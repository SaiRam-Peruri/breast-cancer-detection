{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcff4e9a",
   "metadata": {},
   "source": [
    "# Download CBIS-DDSM Directly to Kaggle (163GB)\n",
    "This notebook downloads the dataset directly to Kaggle storage, bypassing your local machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06c3953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Install TCIA Utils\n",
    "!pip install tcia-utils requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa29feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Import libraries\n",
    "from tcia_utils import nbia\n",
    "import os\n",
    "\n",
    "# Set download directory\n",
    "download_dir = '/kaggle/working/CBIS-DDSM-Full'\n",
    "os.makedirs(download_dir, exist_ok=True)\n",
    "print(f\"Will download to: {download_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddab7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Download CBIS-DDSM in SMALL BATCHES and monitor space\n",
    "# Strategy: Download 500 series at a time (about 12GB) to stay under 57GB limit\n",
    "\n",
    "collection = \"CBIS-DDSM\"\n",
    "print(f\"Downloading collection: {collection}\")\n",
    "\n",
    "# Get series data\n",
    "series_data = nbia.getSeries(collection=collection)\n",
    "total_series = len(series_data)\n",
    "print(f\"Found {total_series} series to download\")\n",
    "\n",
    "# Extract SeriesInstanceUIDs\n",
    "series_uids = [item['SeriesInstanceUID'] for item in series_data]\n",
    "\n",
    "# Download in SMALLER batches to fit in /kaggle/working/ 57GB limit\n",
    "batch_size = 500  # Reduced from 1000 - approximately 12GB per batch\n",
    "total_batches = (total_series + batch_size - 1) // batch_size\n",
    "\n",
    "print(f\"\\nDownloading in {total_batches} batches of {batch_size} series each\")\n",
    "print(\"Each batch ~12GB to stay under Kaggle's 57GB working directory limit\")\n",
    "print(\"⚠️ IMPORTANT: After this completes, IMMEDIATELY save as dataset!\\n\")\n",
    "\n",
    "import subprocess\n",
    "import shutil\n",
    "\n",
    "for batch_num in range(total_batches):\n",
    "    start_idx = batch_num * batch_size\n",
    "    end_idx = min(start_idx + batch_size, total_series)\n",
    "    batch_uids = series_uids[start_idx:end_idx]\n",
    "    \n",
    "    # Check disk space before downloading\n",
    "    disk_usage = shutil.disk_usage('/kaggle/working/')\n",
    "    available_gb = disk_usage.free / (1024**3)\n",
    "    used_gb = disk_usage.used / (1024**3)\n",
    "    \n",
    "    print(f\"Batch {batch_num + 1}/{total_batches}: Downloading series {start_idx + 1} to {end_idx}\")\n",
    "    print(f\"  Disk: {used_gb:.1f}GB used, {available_gb:.1f}GB available\")\n",
    "    \n",
    "    if available_gb < 15:  # Need at least 15GB free for safety\n",
    "        print(f\"  ⚠️ WARNING: Low disk space! Stopping at batch {batch_num + 1}\")\n",
    "        print(f\"  Downloaded {start_idx} of {total_series} series so far\")\n",
    "        print(f\"  You need to SAVE THIS AS DATASET NOW, then continue in a new notebook\")\n",
    "        break\n",
    "    \n",
    "    try:\n",
    "        nbia.downloadSeries(\n",
    "            series_data=batch_uids,\n",
    "            input_type=\"list\",\n",
    "            path=download_dir\n",
    "        )\n",
    "        print(f\"  ✓ Batch {batch_num + 1} complete\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ Error in batch {batch_num + 1}: {e}\")\n",
    "        print(f\"  Downloaded {start_idx} of {total_series} series\")\n",
    "        print(f\"  SAVE AS DATASET NOW before continuing!\")\n",
    "        break\n",
    "\n",
    "print(\"\\n✓ Download phase complete!\")\n",
    "print(f\"⚠️ NEXT: Immediately save this as a Kaggle Dataset (see Cell 5)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9097c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Check download size\n",
    "import subprocess\n",
    "result = subprocess.run(['du', '-sh', download_dir], capture_output=True, text=True)\n",
    "print(f\"Downloaded size: {result.stdout}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85003ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Create Kaggle Dataset from downloaded files\n",
    "# After download completes, you need to save this as a Kaggle Dataset\n",
    "# Go to: File > Save Version > Save & Run All\n",
    "# Then: File > Create Dataset from Notebook Output\n",
    "\n",
    "print(\"\"\"\\n\n",
    "NEXT STEPS:\n",
    "1. Click 'File' > 'Save Version' > 'Save & Run All'\n",
    "2. Wait for notebook to finish running\n",
    "3. Click 'File' > 'Create Dataset from Notebook Output'\n",
    "4. Name it: 'CBIS-DDSM Full Collection'\n",
    "5. This creates a reusable dataset from the downloaded files\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6feaacaa",
   "metadata": {},
   "source": [
    "## After Creating Dataset Above, Use This Training Notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a281ea70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Clone breast cancer detection repo\n",
    "!git clone https://github.com/monajemi-arman/breast_cancer_detection\n",
    "%cd breast_cancer_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e6263d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Install detectron2\n",
    "!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu118/torch2.0/index.html\n",
    "!pip install cloudpickle pydicom xmltodict opencv-python pandas scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5e28be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Setup datasets\n",
    "# Add these as data sources to your notebook:\n",
    "# 1. Your created dataset: cbis-ddsm-full-collection\n",
    "# 2. ramanathansp20/inbreast-dataset\n",
    "# 3. kmader/mias-mammography\n",
    "\n",
    "!mkdir -p datasets\n",
    "!cp -r /kaggle/input/cbis-ddsm-full-collection/* datasets/\n",
    "!cp -r /kaggle/input/inbreast-dataset/* datasets/\n",
    "!cp -r /kaggle/input/mias-mammography/* datasets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41aa3958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Convert datasets\n",
    "!python convert_dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c7d98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Update batch size for 16GB GPU\n",
    "import fileinput\n",
    "\n",
    "with open('detectron.py', 'r') as f:\n",
    "    content = f.read()\n",
    "\n",
    "content = content.replace('batch_size = 1', 'batch_size = 8')\n",
    "content = content.replace('num_workers = 2', 'num_workers = 8')\n",
    "\n",
    "with open('detectron.py', 'w') as f:\n",
    "    f.write(content)\n",
    "\n",
    "print(\"Updated for faster training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22488f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Start training\n",
    "!python detectron.py -c train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dff5a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Download trained model\n",
    "import shutil\n",
    "shutil.copy('output/model_final.pth', '/kaggle/working/')\n",
    "shutil.copy('detectron.cfg.pkl', '/kaggle/working/')\n",
    "print(\"Download model from Output section!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
