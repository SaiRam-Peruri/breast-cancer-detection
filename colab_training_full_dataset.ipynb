{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9753179f",
   "metadata": {},
   "source": [
    "# Train Breast Cancer Detection on Full CBIS-DDSM (152GB)\n",
    "Training with full dataset on Colab Pro - assumes data already downloaded to /content/CBIS-DDSM-Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09427684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Check GPU and verify downloaded data\n",
    "!nvidia-smi\n",
    "import os\n",
    "print(f\"\\nData available: {os.path.exists('/content/CBIS-DDSM-Full')}\")\n",
    "!du -sh /content/CBIS-DDSM-Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02144216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Clone your breast cancer detection repository\n",
    "!git clone https://github.com/SaiRam-Peruri/breast-cancer-detection.git\n",
    "%cd breast-cancer-detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd47467f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Install dependencies\n",
    "!pip install torch torchvision\n",
    "!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu118/torch2.0/index.html\n",
    "!pip install pydicom xmltodict opencv-python pandas scikit-learn cloudpickle pyyaml tqdm pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa78d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Setup datasets and download CSV files\n",
    "import os\n",
    "import shutil\n",
    "import urllib.request\n",
    "\n",
    "# Reset script to clean state\n",
    "!git checkout convert_dataset.py\n",
    "\n",
    "# Create datasets directory\n",
    "os.makedirs('datasets/CBIS-DDSM', exist_ok=True)\n",
    "\n",
    "# Link downloaded data to 'dicom' directory\n",
    "if os.path.exists('datasets/CBIS-DDSM/dicom'):\n",
    "    os.remove('datasets/CBIS-DDSM/dicom')\n",
    "    \n",
    "print(\"Linking dataset...\")\n",
    "!ln -s /content/CBIS-DDSM-Full datasets/CBIS-DDSM/dicom\n",
    "print(\"‚úì Dataset linked\")\n",
    "\n",
    "# Download CSV metadata files from Kaggle (official CBIS-DDSM annotations)\n",
    "print(\"\\nDownloading CSV metadata files from Kaggle...\")\n",
    "csv_dir = 'datasets/CBIS-DDSM/csv'\n",
    "os.makedirs(csv_dir, exist_ok=True)\n",
    "\n",
    "# Install kaggle if needed\n",
    "!pip install -q kaggle\n",
    "\n",
    "# Setup Kaggle API (you'll need your kaggle.json in ~/.kaggle/)\n",
    "# If not authenticated, you'll need to upload kaggle.json\n",
    "import os\n",
    "kaggle_dir = os.path.expanduser('~/.kaggle')\n",
    "os.makedirs(kaggle_dir, exist_ok=True)\n",
    "\n",
    "# Check if kaggle.json exists\n",
    "if not os.path.exists(os.path.join(kaggle_dir, 'kaggle.json')):\n",
    "    print(\"‚ö†Ô∏è Kaggle API key not found!\")\n",
    "    print(\"Please upload your kaggle.json file:\")\n",
    "    print(\"1. Go to https://www.kaggle.com/settings\")\n",
    "    print(\"2. Click 'Create New API Token'\")\n",
    "    print(\"3. Upload the downloaded kaggle.json\")\n",
    "    from google.colab import files\n",
    "    uploaded = files.upload()\n",
    "    if 'kaggle.json' in uploaded:\n",
    "        with open(os.path.join(kaggle_dir, 'kaggle.json'), 'wb') as f:\n",
    "            f.write(uploaded['kaggle.json'])\n",
    "        !chmod 600 ~/.kaggle/kaggle.json\n",
    "        print(\"‚úì Kaggle credentials configured\")\n",
    "\n",
    "# Download CSV files from CBIS-DDSM Kaggle dataset\n",
    "!kaggle datasets download -d awsaf49/cbis-ddsm-breast-cancer-image-dataset --force -p /tmp/cbis_csv\n",
    "\n",
    "# Extract CSVs\n",
    "import zipfile\n",
    "zip_path = '/tmp/cbis_csv/cbis-ddsm-breast-cancer-image-dataset.zip'\n",
    "if os.path.exists(zip_path):\n",
    "    print(\"Inspecting zip contents...\")\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        # List all files to see structure\n",
    "        all_files = zip_ref.namelist()\n",
    "        csv_files = [f for f in all_files if f.endswith('.csv')]\n",
    "        \n",
    "        print(f\"Found {len(csv_files)} CSV files in zip\")\n",
    "        \n",
    "        # Extract all CSV files regardless of path\n",
    "        for csv_file in csv_files:\n",
    "            # Get just the filename (remove any directory path)\n",
    "            filename = os.path.basename(csv_file)\n",
    "            \n",
    "            # Extract to temporary location\n",
    "            zip_ref.extract(csv_file, '/tmp/cbis_csv_extract')\n",
    "            \n",
    "            # Move to our csv_dir with clean filename\n",
    "            src = os.path.join('/tmp/cbis_csv_extract', csv_file)\n",
    "            dst = os.path.join(csv_dir, filename)\n",
    "            shutil.move(src, dst)\n",
    "            print(f\"  ‚úì {filename}\")\n",
    "    \n",
    "    print(\"\\n‚úì CSV files extracted\")\n",
    "    !ls -lh {csv_dir}\n",
    "    \n",
    "    # Cleanup\n",
    "    !rm -rf /tmp/cbis_csv /tmp/cbis_csv_extract\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Download failed - trying manual CSV creation...\")\n",
    "    # Fallback: Create minimal CSV structure for testing\n",
    "    print(\"Creating minimal CSV files for testing...\")\n",
    "\n",
    "# Configure convert_dataset.py\n",
    "print(\"\\nConfiguring convert_dataset.py...\")\n",
    "with open('convert_dataset.py', 'r') as f:\n",
    "    content = f.read()\n",
    "\n",
    "# Update settings for proper train/val/test split (70/15/15)\n",
    "content = content.replace(\n",
    "    \"chosen_datasets = ['inbreast', 'cbis-ddsm', 'mias']\",\n",
    "    \"chosen_datasets = ['cbis-ddsm']\"\n",
    ")\n",
    "content = content.replace(\n",
    "    \"split_ratio = [0.8, 0.1, 0.1]\",\n",
    "    \"split_ratio = [0.7, 0.15, 0.15]\"\n",
    ")\n",
    "\n",
    "with open('convert_dataset.py', 'w') as f:\n",
    "    f.write(content)\n",
    "\n",
    "print(\"‚úì Configuration complete\")\n",
    "print(\"\\nReady for conversion in next cell!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b47922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4.5: Convert all DICOM files to JPEG (like original CBIS-DDSM distribution)\n",
    "# This allows us to use the existing convert_dataset.py without modifications\n",
    "import os\n",
    "import pydicom\n",
    "from tqdm import tqdm\n",
    "from utils import read_dicom\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"Converting DICOM files to JPEG format...\")\n",
    "print(\"This preserves the directory structure for annotation matching\")\n",
    "\n",
    "# Create jpeg output directory\n",
    "jpeg_dir = 'datasets/CBIS-DDSM/jpeg'\n",
    "os.makedirs(jpeg_dir, exist_ok=True)\n",
    "\n",
    "# Scan all DICOM files\n",
    "dicom_files = []\n",
    "for root, dirs, files in os.walk('datasets/CBIS-DDSM/dicom'):\n",
    "    for file in files:\n",
    "        if file.endswith('.dcm'):\n",
    "            dicom_files.append(os.path.join(root, file))\n",
    "\n",
    "print(f\"Found {len(dicom_files)} DICOM files\")\n",
    "print(f\"Converting to JPEG...\\n\")\n",
    "\n",
    "converted = 0\n",
    "failed = 0\n",
    "\n",
    "for dcm_path in tqdm(dicom_files, desc=\"Converting DICOM‚ÜíJPEG\"):\n",
    "    try:\n",
    "        # Read DICOM and convert to image\n",
    "        img = read_dicom(dcm_path)\n",
    "        \n",
    "        # Recreate directory structure\n",
    "        rel_path = os.path.relpath(dcm_path, 'datasets/CBIS-DDSM/dicom')\n",
    "        jpg_path = os.path.join(jpeg_dir, rel_path).replace('.dcm', '.jpg')\n",
    "        \n",
    "        # Create parent directories\n",
    "        os.makedirs(os.path.dirname(jpg_path), exist_ok=True)\n",
    "        \n",
    "        # Save as JPEG\n",
    "        img.convert('RGB').save(jpg_path, 'JPEG', quality=95)\n",
    "        converted += 1\n",
    "        \n",
    "    except Exception as e:\n",
    "        failed += 1\n",
    "        if failed <= 10:  # Show first 10 errors only\n",
    "            print(f\"\\n‚ö†Ô∏è Failed to convert {os.path.basename(dcm_path)}: {str(e)}\")\n",
    "\n",
    "print(f\"\\n‚úì Conversion complete!\")\n",
    "print(f\"  Successfully converted: {converted} files\")\n",
    "print(f\"  Failed: {failed} files\")\n",
    "print(f\"  Output directory: {jpeg_dir}\")\n",
    "\n",
    "# Update convert_dataset.py to use 'jpeg' instead of 'dicom'\n",
    "print(\"\\nUpdating convert_dataset.py to use JPEG directory...\")\n",
    "with open('convert_dataset.py', 'r') as f:\n",
    "    content = f.read()\n",
    "\n",
    "# Change back to jpeg if it was changed to dicom\n",
    "content = content.replace(\n",
    "    \"cbis_jpeg = os.path.join(cbis_path, 'dicom')\",\n",
    "    \"cbis_jpeg = os.path.join(cbis_path, 'jpeg')\"\n",
    ")\n",
    "\n",
    "with open('convert_dataset.py', 'w') as f:\n",
    "    f.write(content)\n",
    "\n",
    "print(\"‚úì Ready for COCO conversion in next cell!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975912c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4.6: Reorganize existing JPEGs to match convert_dataset.py expectations\n",
    "# FASTER: Use existing 10K+ JPEGs instead of converting again (saves 2 hours!)\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from csv import DictReader\n",
    "import shutil\n",
    "\n",
    "print(\"üöÄ Reorganizing existing JPEG files to match convert_dataset.py structure...\")\n",
    "print(\"‚è±Ô∏è This takes ~5-10 minutes instead of 2 hours reconversion!\\n\")\n",
    "\n",
    "# Load dicom_info.csv\n",
    "dicom_info_path = 'datasets/CBIS-DDSM/csv/dicom_info.csv'\n",
    "with open(dicom_info_path) as f:\n",
    "    dicom_info = list(DictReader(f))\n",
    "\n",
    "# Filter non-cropped only\n",
    "dicom_info = [item for item in dicom_info if 'crop' not in item['SeriesDescription']]\n",
    "print(f\"‚úì {len(dicom_info)} non-cropped images to organize\\n\")\n",
    "\n",
    "jpeg_base = 'datasets/CBIS-DDSM/jpeg'\n",
    "\n",
    "# Build a COMPLETE lookup of ALL existing JPEGs with multiple indexing strategies\n",
    "print(\"üìÇ Scanning existing JPEG files...\")\n",
    "existing_jpegs_by_uid = {}\n",
    "all_jpegs_by_filename = {}\n",
    "\n",
    "for root, dirs, files in os.walk(jpeg_base):\n",
    "    for file in files:\n",
    "        if file.endswith('.jpg'):\n",
    "            full_path = os.path.join(root, file)\n",
    "            \n",
    "            # Index by UID\n",
    "            parts = root.split(os.sep)\n",
    "            for part in parts:\n",
    "                if part.startswith('1.3.6.1'):\n",
    "                    if part not in existing_jpegs_by_uid:\n",
    "                        existing_jpegs_by_uid[part] = []\n",
    "                    existing_jpegs_by_uid[part].append(full_path)\n",
    "                    break\n",
    "            \n",
    "            # Also index by just filename for better matching\n",
    "            if file not in all_jpegs_by_filename:\n",
    "                all_jpegs_by_filename[file] = []\n",
    "            all_jpegs_by_filename[file].append(full_path)\n",
    "\n",
    "total_files = sum(len(v) for v in existing_jpegs_by_uid.values())\n",
    "print(f\"‚úì Found {total_files} existing JPEG files\")\n",
    "print(f\"‚úì Covering {len(existing_jpegs_by_uid)} unique UIDs\")\n",
    "print(f\"‚úì Indexed {len(all_jpegs_by_filename)} unique filenames\\n\")\n",
    "\n",
    "# Reorganize based on CSV - using SAME logic as convert_dataset.py\n",
    "moved = 0\n",
    "already_correct = 0\n",
    "missing = 0\n",
    "missing_list = []\n",
    "\n",
    "for item in tqdm(dicom_info, desc=\"Reorganizing JPEGs\"):\n",
    "    # Use EXACT SAME path construction as convert_dataset.py line 340\n",
    "    jpeg_rel_path = os.path.join(*Path(item['image_path'].strip()).parts[-2:])\n",
    "    expected_path = os.path.join(jpeg_base, jpeg_rel_path)\n",
    "    \n",
    "    # Check if already in correct location\n",
    "    if os.path.exists(expected_path):\n",
    "        already_correct += 1\n",
    "        continue\n",
    "    \n",
    "    # Extract UID and filename\n",
    "    parts = Path(item['image_path'].strip()).parts[-2:]\n",
    "    if len(parts) < 2:\n",
    "        missing += 1\n",
    "        continue\n",
    "        \n",
    "    uid = parts[0]\n",
    "    filename = parts[1]\n",
    "    \n",
    "    src_path = None\n",
    "    \n",
    "    # Strategy 1: Match by UID + exact filename\n",
    "    if uid in existing_jpegs_by_uid:\n",
    "        for candidate_path in existing_jpegs_by_uid[uid]:\n",
    "            if candidate_path.endswith(filename):\n",
    "                src_path = candidate_path\n",
    "                break\n",
    "    \n",
    "    # Strategy 2: If UID match failed, try just filename across all UIDs\n",
    "    if not src_path and filename in all_jpegs_by_filename:\n",
    "        # Find the one in the matching UID if possible\n",
    "        for candidate_path in all_jpegs_by_filename[filename]:\n",
    "            if uid in candidate_path:\n",
    "                src_path = candidate_path\n",
    "                break\n",
    "        # Otherwise take first match\n",
    "        if not src_path:\n",
    "            src_path = all_jpegs_by_filename[filename][0]\n",
    "    \n",
    "    if not src_path:\n",
    "        missing += 1\n",
    "        if len(missing_list) < 5:\n",
    "            missing_list.append(f\"{uid}/{filename}\")\n",
    "        continue\n",
    "    \n",
    "    # Create destination directory and copy\n",
    "    os.makedirs(os.path.dirname(expected_path), exist_ok=True)\n",
    "    try:\n",
    "        shutil.copy2(src_path, expected_path)\n",
    "        moved += 1\n",
    "    except Exception as e:\n",
    "        missing += 1\n",
    "        if len(missing_list) < 5:\n",
    "            missing_list.append(f\"{uid}/{filename}: {str(e)[:50]}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Reorganization complete!\")\n",
    "print(f\"  üì¶ Copied to correct location: {moved}\")\n",
    "print(f\"  ‚úì Already in correct location: {already_correct}\")\n",
    "print(f\"  ‚ö†Ô∏è Missing from source: {missing}\")\n",
    "\n",
    "if missing_list:\n",
    "    print(f\"\\n‚ö†Ô∏è Sample missing files:\")\n",
    "    for miss in missing_list:\n",
    "        print(f\"    {miss}\")\n",
    "\n",
    "# Verify using SAME logic as convert_dataset.py\n",
    "print(\"\\nüîç Verifying reorganized files (using convert_dataset.py logic)...\")\n",
    "sample_count = 0\n",
    "verified_missing = []\n",
    "for item in dicom_info[:100]:\n",
    "    jpeg_rel_path = os.path.join(*Path(item['image_path'].strip()).parts[-2:])\n",
    "    full_path = os.path.join(jpeg_base, jpeg_rel_path)\n",
    "    if os.path.exists(full_path):\n",
    "        sample_count += 1\n",
    "    elif len(verified_missing) < 3:\n",
    "        verified_missing.append(jpeg_rel_path)\n",
    "\n",
    "print(f\"‚úì {sample_count}/100 sample files verified at convert_dataset.py expected paths\")\n",
    "\n",
    "if verified_missing:\n",
    "    print(f\"\\n‚ö†Ô∏è Still missing after reorganization:\")\n",
    "    for vm in verified_missing:\n",
    "        print(f\"    {vm}\")\n",
    "else:\n",
    "    print(f\"\\nüéØ All verified! Ready for COCO conversion in Cell 5!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3419d7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4.7: Debug CSV-JPEG matching\n",
    "# Check if CSV files correctly map to JPEG structure\n",
    "\n",
    "print(\"Debugging CSV-JPEG path matching...\\n\")\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from csv import DictReader\n",
    "import os\n",
    "\n",
    "# Check dicom_info.csv\n",
    "dicom_info_path = 'datasets/CBIS-DDSM/csv/dicom_info.csv'\n",
    "if os.path.exists(dicom_info_path):\n",
    "    with open(dicom_info_path) as f:\n",
    "        list_of_dict = list(DictReader(f))\n",
    "    \n",
    "    print(f\"‚úì dicom_info.csv loaded: {len(list_of_dict)} entries\")\n",
    "    \n",
    "    # Show first few entries\n",
    "    print(\"\\nüìã Sample dicom_info.csv entries:\")\n",
    "    for i, item in enumerate(list_of_dict[:3]):\n",
    "        print(f\"\\nEntry {i+1}:\")\n",
    "        print(f\"  file_path: {item.get('file_path', 'N/A')[:80]}\")\n",
    "        print(f\"  image_path: {item.get('image_path', 'N/A')[:80]}\")\n",
    "        print(f\"  SeriesDescription: {item.get('SeriesDescription', 'N/A')}\")\n",
    "    \n",
    "    # Check how many non-crop entries\n",
    "    non_crop = [item for item in list_of_dict if 'crop' not in item['SeriesDescription']]\n",
    "    print(f\"\\n‚úì Non-cropped images: {len(non_crop)}\")\n",
    "    \n",
    "    # Check dcm_jpeg_dict creation\n",
    "    dcm_jpeg_dict = {}\n",
    "    for item in list_of_dict:\n",
    "        if 'crop' not in item['SeriesDescription']:\n",
    "            dcm_path = Path(item['file_path'].strip()).parent.parts[-1]\n",
    "            jpeg_path = os.path.join(*Path(item['image_path'].strip()).parts[-2:])\n",
    "            dcm_jpeg_dict[dcm_path] = jpeg_path\n",
    "    \n",
    "    print(f\"\\n‚úì dcm_jpeg_dict created: {len(dcm_jpeg_dict)} mappings\")\n",
    "    \n",
    "    # Show sample mappings\n",
    "    print(\"\\nüìã Sample DICOM‚ÜíJPEG mappings:\")\n",
    "    for i, (dcm, jpg) in enumerate(list(dcm_jpeg_dict.items())[:3]):\n",
    "        print(f\"  {dcm} ‚Üí {jpg}\")\n",
    "    \n",
    "    # Check if JPEG files actually exist\n",
    "    jpeg_dir = 'datasets/CBIS-DDSM/jpeg'\n",
    "    existing_count = 0\n",
    "    for jpg_path in list(dcm_jpeg_dict.values())[:100]:\n",
    "        full_path = os.path.join(jpeg_dir, jpg_path)\n",
    "        if os.path.exists(full_path):\n",
    "            existing_count += 1\n",
    "    \n",
    "    print(f\"\\n‚úì Sample check: {existing_count}/100 JPEG files exist\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è dicom_info.csv not found!\")\n",
    "\n",
    "# Check mass CSV files\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Checking mass annotation CSVs...\")\n",
    "\n",
    "csv_dir = 'datasets/CBIS-DDSM/csv'\n",
    "mass_csvs = [\n",
    "    'mass_case_description_train_set.csv',\n",
    "    'mass_case_description_test_set.csv'\n",
    "]\n",
    "\n",
    "for csv_name in mass_csvs:\n",
    "    csv_path = os.path.join(csv_dir, csv_name)\n",
    "    if os.path.exists(csv_path):\n",
    "        with open(csv_path) as f:\n",
    "            mass_data = list(DictReader(f))\n",
    "        \n",
    "        print(f\"\\n‚úì {csv_name}: {len(mass_data)} entries\")\n",
    "        \n",
    "        # Show first entry\n",
    "        if mass_data:\n",
    "            item = mass_data[0]\n",
    "            print(f\"\\n  Sample entry:\")\n",
    "            print(f\"    image file path: {item.get('image file path', 'N/A')[:60]}\")\n",
    "            print(f\"    ROI mask file path: {item.get('ROI mask file path', 'N/A')[:60]}\")\n",
    "            print(f\"    assessment: {item.get('assessment', 'N/A')}\")\n",
    "            \n",
    "            # Check patient_dir extraction\n",
    "            patient_dir = Path(item['image file path'].strip()).parent.parts[-1]\n",
    "            print(f\"    extracted patient_dir: {patient_dir}\")\n",
    "            \n",
    "            # Check if it's in dcm_jpeg_dict\n",
    "            if patient_dir in dcm_jpeg_dict:\n",
    "                print(f\"    ‚úì Found in dcm_jpeg_dict ‚Üí {dcm_jpeg_dict[patient_dir]}\")\n",
    "            else:\n",
    "                print(f\"    ‚úó NOT in dcm_jpeg_dict\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è {csv_name} not found!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80b2cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4.8: Cleanup duplicates and broken directories\n",
    "# Run this to free up storage before running Cell 5 again\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "print(\"üßπ Cleaning up storage...\\n\")\n",
    "\n",
    "# 1. Remove old train/val/test directories\n",
    "for dir_name in ['train', 'val', 'test']:\n",
    "    if os.path.exists(dir_name):\n",
    "        size_mb = sum(os.path.getsize(os.path.join(dirpath, f)) \n",
    "                      for dirpath, dirnames, filenames in os.walk(dir_name) \n",
    "                      for f in filenames) / (1024*1024)\n",
    "        print(f\"Removing {dir_name}/ ({size_mb:.1f} MB)...\")\n",
    "        shutil.rmtree(dir_name)\n",
    "\n",
    "# 2. Remove old JSON files\n",
    "for file_name in ['train.json', 'val.json', 'test.json', 'dataset.yaml']:\n",
    "    if os.path.exists(file_name):\n",
    "        print(f\"Removing {file_name}...\")\n",
    "        os.remove(file_name)\n",
    "\n",
    "# 3. Remove old images/labels directories (will be recreated)\n",
    "for dir_name in ['images', 'labels']:\n",
    "    if os.path.exists(dir_name):\n",
    "        size_mb = sum(os.path.getsize(os.path.join(dirpath, f)) \n",
    "                      for dirpath, dirnames, filenames in os.walk(dir_name) \n",
    "                      for f in filenames) / (1024*1024)\n",
    "        print(f\"Removing {dir_name}/ ({size_mb:.1f} MB)...\")\n",
    "        shutil.rmtree(dir_name)\n",
    "\n",
    "# 4. Find and remove empty directories in jpeg/\n",
    "jpeg_dir = 'datasets/CBIS-DDSM/jpeg'\n",
    "removed_dirs = 0\n",
    "if os.path.exists(jpeg_dir):\n",
    "    for root, dirs, files in os.walk(jpeg_dir, topdown=False):\n",
    "        for dir_name in dirs:\n",
    "            dir_path = os.path.join(root, dir_name)\n",
    "            try:\n",
    "                if not os.listdir(dir_path):  # Empty directory\n",
    "                    os.rmdir(dir_path)\n",
    "                    removed_dirs += 1\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "if removed_dirs > 0:\n",
    "    print(f\"Removed {removed_dirs} empty directories from jpeg/\\n\")\n",
    "\n",
    "# 5. Check current storage usage\n",
    "jpeg_size = sum(os.path.getsize(os.path.join(dirpath, f)) \n",
    "                for dirpath, dirnames, filenames in os.walk(jpeg_dir) \n",
    "                for f in filenames if f.endswith('.jpg')) / (1024*1024*1024)\n",
    "\n",
    "print(f\"\\n‚úÖ Cleanup complete!\")\n",
    "print(f\"üìä Current JPEG storage: {jpeg_size:.2f} GB\")\n",
    "print(f\"üéØ Ready for fresh conversion in Cell 5!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06fffd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4.9: Re-configure for 70/15/15 split (if you already ran Cell 4 with old config)\n",
    "# Only run this if you need to update from 70/0/30 to 70/15/15 split\n",
    "\n",
    "print(\"Updating convert_dataset.py to 70/15/15 split...\")\n",
    "with open('convert_dataset.py', 'r') as f:\n",
    "    content = f.read()\n",
    "\n",
    "# Update to proper split ratio\n",
    "content = content.replace(\n",
    "    \"split_ratio = [0.7, 0.0, 0.3]\",\n",
    "    \"split_ratio = [0.7, 0.15, 0.15]\"\n",
    ")\n",
    "# Also ensure it's not the default ratio\n",
    "content = content.replace(\n",
    "    \"split_ratio = [0.8, 0.1, 0.1]\",\n",
    "    \"split_ratio = [0.7, 0.15, 0.15]\"\n",
    ")\n",
    "\n",
    "with open('convert_dataset.py', 'w') as f:\n",
    "    f.write(content)\n",
    "\n",
    "print(\"‚úì Split ratio updated to [0.7, 0.15, 0.15]\")\n",
    "print(\"\\nNow run Cell 5 to regenerate JSON files with correct split!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b994223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Convert JPEG images + CSV annotations to COCO format (ALL IMAGES: Mass + Calcifications)\n",
    "# ‚ö†Ô∏è Run Cell 4.8 first to cleanup and free storage!\n",
    "# This will take ~20-30 minutes since images are already converted to JPEG\n",
    "import time\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "print(\"Converting CBIS-DDSM to COCO format...\")\n",
    "print(\"Dataset: ~3,100 annotated images (Mass + Calcification)\")\n",
    "print(\"Classes: Mass AND Calcification (all abnormalities)\")\n",
    "print(\"Split: 70% train, 15% val, 15% test\")\n",
    "print(\"This will take 20-30 minutes...\\n\")\n",
    "\n",
    "# Enable BOTH mass and calcification classes\n",
    "print(\"Configuring for ALL classes (mass + calcification)...\")\n",
    "with open('convert_dataset.py', 'r') as f:\n",
    "    content = f.read()\n",
    "content = content.replace(\n",
    "    \"chosen_classes = ['mass']\",\n",
    "    \"chosen_classes = ['mass', 'calcification']\"\n",
    ")\n",
    "# Disable offline augmentation (faster)\n",
    "content = content.replace(\n",
    "    'offline_augmentation_enabled = True',\n",
    "    'offline_augmentation_enabled = False'\n",
    ")\n",
    "with open('convert_dataset.py', 'w') as f:\n",
    "    f.write(content)\n",
    "print(\"‚úì Both mass and calcification enabled\\n\")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Run conversion - answer 'y' for mass_low/mass_high classes\n",
    "!echo \"y\" | python convert_dataset.py\n",
    "\n",
    "elapsed = (time.time() - start) / 60\n",
    "print(f\"\\n‚úì Conversion complete in {elapsed:.1f} minutes!\")\n",
    "\n",
    "# Verify output - Check all three splits\n",
    "if os.path.exists('train.json') and os.path.exists('val.json') and os.path.exists('test.json'):\n",
    "    with open('train.json') as f:\n",
    "        train = json.load(f)\n",
    "    with open('val.json') as f:\n",
    "        val = json.load(f)\n",
    "    with open('test.json') as f:\n",
    "        test = json.load(f)\n",
    "    \n",
    "    print(\"\\nüìä Dataset Statistics:\")\n",
    "    print(f\"  Train: {len(train['images'])} images, {len(train['annotations'])} annotations\")\n",
    "    print(f\"  Val:   {len(val['images'])} images, {len(val['annotations'])} annotations\")\n",
    "    print(f\"  Test:  {len(test['images'])} images, {len(test['annotations'])} annotations\")\n",
    "    print(f\"  Categories: {train['categories']}\")\n",
    "    \n",
    "    # Count annotations per class\n",
    "    train_class_counts = {}\n",
    "    for ann in train['annotations']:\n",
    "        cat_id = ann['category_id']\n",
    "        cat_name = next(c['name'] for c in train['categories'] if c['id'] == cat_id)\n",
    "        train_class_counts[cat_name] = train_class_counts.get(cat_name, 0) + 1\n",
    "    \n",
    "    print(f\"\\nüìä Class distribution (train):\")\n",
    "    for cls, count in sorted(train_class_counts.items()):\n",
    "        print(f\"  {cls}: {count} annotations\")\n",
    "    \n",
    "    # Verify split ratios\n",
    "    total = len(train['images']) + len(val['images']) + len(test['images'])\n",
    "    if total > 0:\n",
    "        print(f\"\\n‚úì Split verification:\")\n",
    "        print(f\"  Train: {len(train['images'])/total*100:.1f}% ({len(train['images'])} images)\")\n",
    "        print(f\"  Val:   {len(val['images'])/total*100:.1f}% ({len(val['images'])} images)\")\n",
    "        print(f\"  Test:  {len(test['images'])/total*100:.1f}% ({len(test['images'])} images)\")\n",
    "        print(f\"\\nüéØ Total images processed: {total}\")\n",
    "        print(f\"‚ÑπÔ∏è  This is correct - only annotated images are used for training\")\n",
    "        print(f\"‚ÑπÔ∏è  The remaining ~3,500 files are mask images (not for training)\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è ERROR: No images were processed!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Conversion failed - JSON files not found\")\n",
    "    print(\"Missing files:\")\n",
    "    if not os.path.exists('train.json'): print(\"  - train.json\")\n",
    "    if not os.path.exists('val.json'): print(\"  - val.json\")\n",
    "    if not os.path.exists('test.json'): print(\"  - test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07f0eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Configure detectron2 for Colab Pro GPU\n",
    "# Optimize batch size and workers for better GPU utilization\n",
    "\n",
    "with open('detectron.py', 'r') as f:\n",
    "    content = f.read()\n",
    "\n",
    "# Update for Colab Pro (potentially A100 or V100)\n",
    "content = content.replace('batch_size = 1', 'batch_size = 8')  # Larger batch for better GPU\n",
    "content = content.replace('num_workers = 2', 'num_workers = 4')\n",
    "content = content.replace('epochs = 100', 'epochs = 150')  # More epochs for full dataset\n",
    "\n",
    "# Enable checkpointing every 5000 iterations\n",
    "if 'cfg.SOLVER.CHECKPOINT_PERIOD' not in content:\n",
    "    # Add checkpoint period if not exists\n",
    "    content = content.replace(\n",
    "        'cfg.SOLVER.MAX_ITER',\n",
    "        'cfg.SOLVER.CHECKPOINT_PERIOD = 5000  # Save every 5000 iterations\\n    cfg.SOLVER.MAX_ITER'\n",
    "    )\n",
    "\n",
    "# Write updated content\n",
    "with open('detectron.py', 'w') as f:\n",
    "    f.write(content)\n",
    "\n",
    "print(\"‚úì detectron.py configured:\")\n",
    "print(\"  - Batch size: 8\")\n",
    "print(\"  - Workers: 4\")\n",
    "print(\"  - Epochs: 150\")\n",
    "print(\"  - Checkpoint every 5000 iterations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed845b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Start training!\n",
    "# This will run for ~6-10 hours with A100 GPU\n",
    "# Model checkpoints saved in output/ directory every 5000 iterations\n",
    "\n",
    "import time\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STARTING TRAINING ON FULL CBIS-DDSM DATASET\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Dataset: 152GB, ~6,750 series\")\n",
    "print(f\"Model: Faster R-CNN with ResNet-50-FPN\")\n",
    "print(f\"Expected duration: 6-10 hours\")\n",
    "print(f\"Checkpoints: output/model_XXXX.pth (every 5000 iterations)\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "!python detectron.py -c train\n",
    "\n",
    "elapsed_hours = (time.time() - start) / 3600\n",
    "print(f\"\\n‚úì Training complete in {elapsed_hours:.1f} hours!\")\n",
    "print(\"\\nSaved models:\")\n",
    "!ls -lh output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef578904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Evaluate model on test set\n",
    "!python detectron.py -c test\n",
    "\n",
    "print(\"\\nTest results saved!\")\n",
    "!cat output/test_results.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b15aedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Download trained model\n",
    "from google.colab import files\n",
    "import shutil\n",
    "import datetime\n",
    "\n",
    "\n",
    "# Create timestampprint(\"  - All checkpoints\")\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M\")print(\"  - metrics.json (training metrics)\")\n",
    "\n",
    "model_name = f\"breast_cancer_full_dataset_{timestamp}.zip\"print(\"  - detectron.cfg.pkl (config)\")\n",
    "\n",
    "print(\"  - model_final.pth (trained weights)\")\n",
    "\n",
    "# Zip output folderprint(\"Files included:\")\n",
    "\n",
    "print(f\"Creating {model_name}...\")print(\"\\n‚úì Model downloaded!\")\n",
    "\n",
    "shutil.make_archive(model_name.replace('.zip', ''), 'zip', 'output')\n",
    "\n",
    "files.download(model_name)\n",
    "print(f\"\\nDownloading model...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a143ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Download trained model\n",
    "from google.colab import files\n",
    "import shutil\n",
    "import datetime\n",
    "\n",
    "# Create timestamp\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "model_name = f\"breast_cancer_full_dataset_{timestamp}.zip\"\n",
    "\n",
    "# Zip output folder\n",
    "print(f\"Creating {model_name}...\")\n",
    "shutil.make_archive(model_name.replace('.zip', ''), 'zip', 'output')\n",
    "\n",
    "print(f\"\\nDownloading model...\")\n",
    "files.download(model_name)\n",
    "\n",
    "print(\"\\n‚úì Model downloaded!\")\n",
    "print(\"Files included:\")\n",
    "print(\"  - model_final.pth (trained weights)\")\n",
    "print(\"  - detectron.cfg.pkl (config)\")\n",
    "print(\"  - metrics.json (training metrics)\")\n",
    "print(\"  - All checkpoints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e7e34a",
   "metadata": {},
   "source": [
    "## Training Complete! üéâ\n",
    "\n",
    "Your model has been trained on the full 152GB CBIS-DDSM dataset with:\n",
    "- ~4,725 training images (70%)\n",
    "- ~2,025 test images (30%)\n",
    "- 150 epochs\n",
    "- Faster R-CNN architecture\n",
    "\n",
    "The model is now ready to use for breast cancer detection in mammograms!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffc4618",
   "metadata": {},
   "source": [
    "## ‚ö° Performance Optimization Tips (For Next Training Run)\n",
    "\n",
    "Your A100 GPU is powerful but currently **78% bottlenecked by disk I/O** (8 sec data loading vs 10 sec total).\n",
    "\n",
    "### Why Training Takes 8-9 Hours:\n",
    "- **Data loading**: 8 seconds/iteration (disk bottleneck)\n",
    "- **GPU compute**: Only 2 seconds/iteration (A100 is fast!)\n",
    "- **Total iterations**: ~40,000+ iterations for 150 epochs\n",
    "- **Object detection**: 4 loss functions (more complex than classification)\n",
    "\n",
    "### Speed Improvements (Could reduce to 4-5 hours):\n",
    "\n",
    "1. **Increase batch_size** (A100 has 80GB VRAM!)\n",
    "   ```python\n",
    "   batch_size = 16  # or even 24-32 with A100\n",
    "   ```\n",
    "   Currently using only 48GB/80GB\n",
    "\n",
    "2. **More workers** for data loading\n",
    "   ```python\n",
    "   num_workers = 8  # or 12\n",
    "   ```\n",
    "\n",
    "3. **Reduce image resolution** (if acceptable)\n",
    "   ```python\n",
    "   cfg.INPUT.MIN_SIZE_TRAIN = (600,)  # instead of default 800\n",
    "   ```\n",
    "\n",
    "4. **Enable caching** (load all images to RAM once)\n",
    "   ```python\n",
    "   # In convert_dataset.py: use in-memory dataset\n",
    "   ```\n",
    "\n",
    "5. **Use SSD storage** instead of network storage\n",
    "\n",
    "### Current Training is Normal:\n",
    "‚úÖ Loss decreasing (0.358 is good!)  \n",
    "‚úÖ GPU memory usage healthy (48/80 GB)  \n",
    "‚úÖ A100 working correctly  \n",
    "\n",
    "The **disk I/O bottleneck is typical for Colab** with large medical imaging datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6992e766",
   "metadata": {},
   "source": [
    "## üöÄ Alternative: Mount Kaggle Dataset in Colab (Faster I/O)\n",
    "\n",
    "Instead of downloading 152GB to Colab storage, mount Kaggle dataset directly:\n",
    "\n",
    "**Pros:**\n",
    "- Faster I/O (Kaggle's optimized storage)\n",
    "- Keep A100 GPU speed\n",
    "- Could reduce data loading from 8 sec ‚Üí 4-5 sec\n",
    "- Training might finish in **6-7 hours** instead of 8-9\n",
    "\n",
    "**How to do it (for next run):**\n",
    "1. Install Kaggle API in Colab\n",
    "2. Use `kagglehub` to mount dataset\n",
    "3. Point training to mounted path\n",
    "\n",
    "This gives you **best of both worlds**: A100 speed + Kaggle storage speed!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
