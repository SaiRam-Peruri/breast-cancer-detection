{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9753179f",
   "metadata": {},
   "source": [
    "# Train Breast Cancer Detection on Full CBIS-DDSM (152GB)\n",
    "Training with full dataset on Colab Pro - assumes data already downloaded to /content/CBIS-DDSM-Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09427684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Check GPU and verify downloaded data\n",
    "!nvidia-smi\n",
    "import os\n",
    "print(f\"\\nData available: {os.path.exists('/content/CBIS-DDSM-Full')}\")\n",
    "!du -sh /content/CBIS-DDSM-Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02144216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Clone your breast cancer detection repository\n",
    "!git clone https://github.com/monajemi-arman/breast_cancer_detection.git\n",
    "%cd breast_cancer_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd47467f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Install dependencies\n",
    "!pip install torch torchvision\n",
    "!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu118/torch2.0/index.html\n",
    "!pip install pydicom xmltodict opencv-python pandas scikit-learn cloudpickle pyyaml tqdm pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa78d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Setup datasets directory structure\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Create datasets directory\n",
    "os.makedirs('datasets/CBIS-DDSM', exist_ok=True)\n",
    "\n",
    "# Move downloaded data to expected location\n",
    "print(\"Moving CBIS-DDSM data to datasets folder...\")\n",
    "print(\"This creates symlink to avoid copying 152GB\")\n",
    "\n",
    "# Create symlink instead of copy to save space\n",
    "!ln -s /content/CBIS-DDSM-Full datasets/CBIS-DDSM/dicom\n",
    "\n",
    "print(\"âœ“ Dataset linked successfully\")\n",
    "!ls -lh datasets/CBIS-DDSM/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3419d7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Download CBIS-DDSM CSV metadata files\n",
    "# These are needed for convert_dataset.py to work\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "csv_dir = 'datasets/CBIS-DDSM/csv'\n",
    "os.makedirs(csv_dir, exist_ok=True)\n",
    "\n",
    "csv_files = [\n",
    "    'calc_case_description_test_set.csv',\n",
    "    'calc_case_description_train_set.csv',\n",
    "    'mass_case_description_test_set.csv',\n",
    "    'mass_case_description_train_set.csv',\n",
    "    'dicom_info.csv'\n",
    "]\n",
    "\n",
    "base_url = 'https://raw.githubusercontent.com/monajemi-arman/breast_cancer_detection/master/datasets/CBIS-DDSM/csv/'\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    url = base_url + csv_file\n",
    "    dest = os.path.join(csv_dir, csv_file)\n",
    "    if not os.path.exists(dest):\n",
    "        print(f\"Downloading {csv_file}...\")\n",
    "        urllib.request.urlretrieve(url, dest)\n",
    "        print(f\"  âœ“ {csv_file}\")\n",
    "\n",
    "print(\"\\nâœ“ All CSV files ready\")\n",
    "!ls -lh {csv_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07f0eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Modify convert_dataset.py for Colab environment\n",
    "# Update paths to work with our data structure\n",
    "\n",
    "with open('convert_dataset.py', 'r') as f:\n",
    "    content = f.read()\n",
    "\n",
    "# Update CBIS-DDSM path to point to our linked data\n",
    "content = content.replace(\n",
    "    \"cbis_jpeg = os.path.join(cbis_path, 'jpeg')\",\n",
    "    \"cbis_jpeg = os.path.join(cbis_path, 'dicom')  # Changed from jpeg to dicom\"\n",
    ")\n",
    "\n",
    "# Force only CBIS-DDSM dataset (skip INbreast and MIAS for now)\n",
    "content = content.replace(\n",
    "    \"chosen_datasets = ['inbreast', 'cbis-ddsm', 'mias']\",\n",
    "    \"chosen_datasets = ['cbis-ddsm']  # Only CBIS-DDSM for full dataset training\"\n",
    ")\n",
    "\n",
    "# Set train/test split to 70/30 (no validation for now)\n",
    "content = content.replace(\n",
    "    \"split_ratio = [0.8, 0.1, 0.1]\",\n",
    "    \"split_ratio = [0.7, 0.0, 0.3]  # 70% train, 30% test\"\n",
    ")\n",
    "\n",
    "with open('convert_dataset.py', 'w') as f:\n",
    "    f.write(content)\n",
    "\n",
    "print(\"âœ“ convert_dataset.py configured for Colab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f231eacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Convert DICOM to COCO format\n",
    "# This will take ~30-60 minutes for the full 152GB dataset\n",
    "# Creates train.json, test.json with 70/30 split\n",
    "\n",
    "import sys\n",
    "import time\n",
    "\n",
    "print(\"Converting CBIS-DDSM to COCO format...\")\n",
    "print(\"Dataset: 152GB, ~6,750 series\")\n",
    "print(\"Split: 70% train, 30% test\")\n",
    "print(\"This will take 30-60 minutes...\\n\")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Run conversion - answer 'y' for mass_low/mass_high classes\n",
    "!echo \"y\" | python convert_dataset.py\n",
    "\n",
    "elapsed = (time.time() - start) / 60\n",
    "print(f\"\\nâœ“ Conversion complete in {elapsed:.1f} minutes!\")\n",
    "print(\"\\nGenerated files:\")\n",
    "!ls -lh *.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d091bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Verify dataset split\n",
    "import json\n",
    "\n",
    "with open('train.json', 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "    \n",
    "with open('test.json', 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "print(\"Dataset Statistics:\")\n",
    "print(f\"  Train images: {len(train_data['images'])}\")\n",
    "print(f\"  Train annotations: {len(train_data['annotations'])}\")\n",
    "print(f\"  Test images: {len(test_data['images'])}\")\n",
    "print(f\"  Test annotations: {len(test_data['annotations'])}\")\n",
    "print(f\"\\n  Total images: {len(train_data['images']) + len(test_data['images'])}\")\n",
    "print(f\"  Train/Test ratio: {len(train_data['images']) / (len(train_data['images']) + len(test_data['images'])) * 100:.1f}% / {len(test_data['images']) / (len(train_data['images']) + len(test_data['images'])) * 100:.1f}%\")\n",
    "print(f\"\\n  Categories: {train_data['categories']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed845b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Configure detectron2 for Colab Pro GPU\n",
    "# Optimize batch size and workers for better GPU utilization\n",
    "\n",
    "with open('detectron.py', 'r') as f:\n",
    "    content = f.read()\n",
    "\n",
    "# Update for Colab Pro (potentially A100 or V100)\n",
    "content = content.replace('batch_size = 1', 'batch_size = 8')  # Larger batch for better GPU\n",
    "content = content.replace('num_workers = 2', 'num_workers = 4')\n",
    "content = content.replace('epochs = 100', 'epochs = 150')  # More epochs for full dataset\n",
    "\n",
    "# Enable checkpointing every 5000 iterations\n",
    "if 'cfg.SOLVER.CHECKPOINT_PERIOD' not in content:\n",
    "    # Add checkpoint period if not exists\n",
    "    content = content.replace(\n",
    "        'cfg.SOLVER.MAX_ITER',\n",
    "        'cfg.SOLVER.CHECKPOINT_PERIOD = 5000  # Save every 5000 iterations\\n    cfg.SOLVER.MAX_ITER'\n",
    "    )\n",
    "\n",
    "with open('detectron.py', 'w') as f:\n",
    "    f.write(content)\n",
    "\n",
    "print(\"âœ“ detectron.py configured:\")\n",
    "print(\"  - Batch size: 8\")\n",
    "print(\"  - Workers: 4\")\n",
    "print(\"  - Epochs: 150\")\n",
    "print(\"  - Checkpoint every 5000 iterations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef578904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Start training!\n",
    "# This will run for ~12-18 hours\n",
    "# Model checkpoints saved in output/ directory every 5000 iterations\n",
    "\n",
    "import time\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STARTING TRAINING ON FULL CBIS-DDSM DATASET\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Dataset: 152GB, ~6,750 series\")\n",
    "print(f\"Model: Faster R-CNN with ResNet-50-FPN\")\n",
    "print(f\"Expected duration: 12-18 hours\")\n",
    "print(f\"Checkpoints: output/model_XXXX.pth (every 5000 iterations)\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "!python detectron.py -c train\n",
    "\n",
    "elapsed_hours = (time.time() - start) / 3600\n",
    "print(f\"\\nâœ“ Training complete in {elapsed_hours:.1f} hours!\")\n",
    "print(\"\\nSaved models:\")\n",
    "!ls -lh output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b15aedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Evaluate model on test set\n",
    "!python detectron.py -c test\n",
    "\n",
    "print(\"\\nTest results saved!\")\n",
    "!cat output/test_results.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a143ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Download trained model\n",
    "from google.colab import files\n",
    "import shutil\n",
    "import datetime\n",
    "\n",
    "# Create timestamp\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "model_name = f\"breast_cancer_full_dataset_{timestamp}.zip\"\n",
    "\n",
    "# Zip output folder\n",
    "print(f\"Creating {model_name}...\")\n",
    "shutil.make_archive(model_name.replace('.zip', ''), 'zip', 'output')\n",
    "\n",
    "print(f\"\\nDownloading model...\")\n",
    "files.download(model_name)\n",
    "\n",
    "print(\"\\nâœ“ Model downloaded!\")\n",
    "print(\"Files included:\")\n",
    "print(\"  - model_final.pth (trained weights)\")\n",
    "print(\"  - detectron.cfg.pkl (config)\")\n",
    "print(\"  - metrics.json (training metrics)\")\n",
    "print(\"  - All checkpoints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e7e34a",
   "metadata": {},
   "source": [
    "## Training Complete! ðŸŽ‰\n",
    "\n",
    "Your model has been trained on the full 152GB CBIS-DDSM dataset with:\n",
    "- ~4,725 training images (70%)\n",
    "- ~2,025 test images (30%)\n",
    "- 150 epochs\n",
    "- Faster R-CNN architecture\n",
    "\n",
    "The model is now ready to use for breast cancer detection in mammograms!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
